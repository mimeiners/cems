# Delta-Sigma ADCs

The general block diagram of a Delta-Sigma ADC is as follows:

![General block diagram of a Delta-sigma ADC](figures/General_block_diagram.drawio.svg){#fig-General-block-diagram-of-a-Delta-sigma-ADC width=105%}

The topology includes an antialiasing filter, which band-limits the analog input signal to prevent aliasing during subsequent sampling. In this case, the sampling is oversampling and significantly reduces the attenuation requirements of the AAF, allowing for smoother transition bands compared to Nyquist-rate ADCs. The central block, the Delta-Sigma modulator, performs oversampling and quantization of the band-limited analog signal. The quantization noise is shaped in the frequency domain by introducing an appropriate loop filter $H(z)$ and enclosing it within a negative feedback loop. The decimation filter applies a highly selective digital filter to sharply remove out-of-band spectral components from the delta-sigma output, effectively eliminating most of the shaped quantization noise. Additionally, the decimator reduces the data rate from $f_s$ down to the Nyquist frequency while increasing the word length [@delarosa2013].

## System Outline

<!-- ![System overview](figures/System_overview.drawio.svg){#fig-Sysem-overview width=85%} -->

A $\Delta\Sigma$ converter operates as a 1-bit sampling system, where an analog input signal is sampled multiple times through oversampling. At its core, the $\Delta\Sigma$ modulator converts the analog signal into a high-speed, single-bit pulse stream while shaping the quantization noise. The modulator can be analyzed in both the time domain and frequency domain.

![First-order $\Delta\Sigma$ modulator in the time domain [@baker2011delta]](figures/first_order_time_domain.drawio.svg){#fig-first_order_delta_sigma_modulator_time_domain width=85%}

@fig-first_order_delta_sigma_modulator_time_domain depicts the time-domain representation of the modulator. It consists of a difference amplifier, an integrator, and a comparator, with a feedback loop incorporating a 1-bit DAC.

The system continuously calculates the difference between the input signal and a feedback signal, emphasizing changes rather than absolute values. This difference signal is then integrated, influencing the output direction. When the integrator output reaches the comparator threshold, the 1-bit ADC quantizes it into a binary value. The output is fed back through a 1-bit DAC, creating a closed-loop system that shapes the noise and stabilizes the signal. The resulting pulse waveform represents the input signal, and when averaged over time, it approximates the original analog input [@baker2011delta]. The DAC functions as a simple switch that connects the negative input of the difference amplifier to either a positive or negative reference voltage. Its primary role is to keep the integrator's average output near the comparator's reference level [@integrated2003demystifying].

![First-order $\Delta\Sigma$ modulator in the frequency domain](figures/first_order_freq_domain1.drawio.svg){#fig-first_order_delta_sigma_modulator_frequency_domain width=85%}

@fig-first_order_delta_sigma_modulator_frequency_domain shows the frequency-domain representation. In this domain, the combination of integration and oversampling functions as a noise-shaping filter, enabling high-resolution conversion by pushing noise outside the signal band. Thanks to these characteristics, $\Delta\Sigma$ ADCs are widely used in applications requiring high accuracy, such as audio processing, sensor technology, and precision measurements .

::: {.callout-note title="Why is it called Delta-Sigma?"}

The name **Delta-Sigma** originates from the key mathematical operations performed in the modulator.  

- **Delta (Δ)** represents the difference operation. The system continuously calculates the difference between the input signal and the feedback signal, ensuring that it tracks changes rather than absolute values. This differentiation process helps in shaping the quantization noise.  
- **Sigma (Σ)** refers to accumulation (or integration). After computing the difference, the error signal is integrated over time, effectively summing past values. This process shapes the noise spectrum, reducing in-band noise while pushing most of it to higher frequencies.  

Some sources use the term **Sigma-Delta** instead of **Delta-Sigma**, emphasizing the integration first. However, both terms describe the same modulation technique.

:::

## Fundamental Concepts

![Fundamental concepts of the $\Delta\Sigma$ ADC](figures/concepts_delta_sigma_adc.drawio.svg){#fig-Fundamental-concepts-of-the-Delta-Sigma-ADC}

A $\Delta\Sigma$ ADC leverages advanced signal processing techniques to achieve higher resolution than conventional ADCs. Oversampling increases the sampling rate beyond the Nyquist frequency, spreading quantization noise over a wider frequency range. Noise shaping further pushes this noise into higher frequencies, where it can be effectively removed by digital filtering. Finally, decimation reduces the sample rate while preserving the signal's integrity, resulting in a high-precision digital output.

## Oversampling

If an ADC operates faster than $2f_{\text{max}}$, it is considered an oversampled ADC. In this case, the **oversampling ratio (OSR)** is a design parameter that indicates how many times $f_s$ is larger than the minimal value required by the Nyquist theorem.

$$
OSR = \frac{f_s}{2 f_{\text{max}}}, \tag{2}
$$

where $f_{\text{max}}$ is the maximum signal frequency, which is the signal bandwidth [@pavan2017understanding]. 

Oversampled ADCs impose significantly less stringent requirements on the filter compared to Nyquist rate converters, as the signal is sampled at a frequency that far exceeds its bandwidth [@Boser1988]. One advantage of oversampling ADCs is that they simplify the requirements for the AAF, as can be seen in @fig-Antialiasing-filter-for-oversampling-ADCs. It is important to note that the AAF for a Nyquist converter must have a sharp transition band, which often results in phase distortion for signal components near the cut-off frequency [@delarosa2011sigmadelta], as can be seen in @fig-Reconstruction-by-filtering-with-a-low-pass-filter.

![Antialiasing filter for oversampling ADCs](figures/oversampled_spectrum.drawio.svg){#fig-Antialiasing-filter-for-oversampling-ADCs width=100%}

However, oversampling has another effect that must be considered: noise. This phenomenon, known as quantization noise, arises for the following reason: while the ADC input is a continuous signal with an infinite range of possible values, the digital output is discrete, with the number of distinct states determined by the converter’s resolution. As a result, the analog-to-digital conversion process inevitably discards some information, introducing a certain degree of distortion into the signal. The associated error varies randomly, with a magnitude of up to ±1 LSB [@Clifford2016]. @fig-Quantization-noise-in-Nyquist-rate-ADCs shows the signal with quantization noise:

![Quantization noise in Nyquist-rate ADCs](figures/spectrum_repetition.drawio.svg){#fig-Quantization-noise-in-Nyquist-rate-ADCs width=75%}

In practice, an ideal quantizer is often described using a linear model, provided that certain assumptions about the statistical properties of the quantization error hold. The quantization error $e(n)$ is directly dependent on the quantizer input signal $q(n)$, as shown in @fig-Multi-bit-quantization-error [@delarosa2013].

![Multi-bit quantization error](figures/Multi_bit_quantization.drawio.svg){#fig-Multi-bit-quantization-error width=80%}

If $q(n)$ is considered to vary randomly from sample to sample within the interval $[-\Delta/2, +\Delta/2]$, the quantization error $e(n)$ will also be uncorrelated between successive samples. Under these assumptions, the quantization error can be represented as a random process following a uniform probability distribution within the range $[-\Delta/2, +\Delta/2]$, as shown in @fig-Probability-density-function [@delarosa2013].

![Probability density function (PDF) of the quantization error assuming a uniform distribution](figures/PDF.drawio.svg){#fig-Probability-density-function width=60%}

As can be seen, quantization noise exhibits characteristics similar to white noise in the frequency domain, maintaining a constant power spectral density across the entire frequency range. The power associated with the quantization error can thus be computed as [@delarosa2013]:

$$
\overline{e^2} = \sigma_e^2 = \int_{-\infty}^{\infty} e^2 \, \text{PDF}(e) \, de = \frac{1}{\Delta} \int_{-\Delta/2}^{+\Delta/2} e^2 \, de = \frac{\Delta^2}{12}. \tag{3}
$$

The former assumption implies that the power of the quantization error will also be uniformly distributed in the range $[-f_s/2, +f_s/2]$, yielding [@delarosa2013]:

$$
\overline{e^2} = \int_{-\infty}^{\infty} S_E(f) \, df = S_E \int_{-f_s/2}^{+f_s/2} df = \frac{\Delta^2}{12}, \tag{4}
$$

so that the PSD of the quantization error in this range is:

$$
S_E = \frac{\overline{e^2}}{f_s} = \frac{\Delta^2}{12 f_s}.\tag{5}
$$

These assumptions are referred to as the additive white noise approximation of the quantization error. They allow a quantizer, which is inherently deterministic and nonlinear, to be represented using a random linear model. This model is expressed as [@delarosa2013]:

$$
y(n) = k_q(n) + e(n) \tag{6} ,
$$

where $e(n)$ represents the quantization noise.  

Using this approximation of quantization error as white noise, the performance of ideal ADCs can be easily evaluated. In a Nyquist ADC, where $f_s = 2 f_{\text{max}}$, all the quantization noise power remains within the signal band and appears at the ADC output as part of the input signal itself, as illustrated in @fig-Quantization-noise-in-Nyquist-rate-ADCs [@integrated2003demystifying].

Conversely, if an oversampled signal is quantized, only a fraction of the total quantization noise power lies within the signal band, as illustrated in @fig-Quantization-noise-in-Nyquist-rate-ADCs:

![Quantization noise in oversampling ADCs](figures/oversampled_spectrum_noise.drawio.svg){#fig-Quantization-noise-in-Nyquist-rate-ADCs width=100%}

While the SNR remains unchanged, the noise energy is now distributed across a broader frequency range [@integrated2003demystifying]. The in-band noise power (IBN) caused by the quantization process in an ideal oversampling ADC is thus [@pavan2017understanding]:

$$
\text{IBN} = \int_{-f_{\text{max}}}^{+f_{\text{max}}} S_E(f) \, df = \int_{-f_{\text{max}}}^{+f_{\text{max}}} \frac{\Delta^2}{12 f_s} \, df = \frac{\Delta^2}{12 \text{OSR}} \tag{7},
$$
where $S_E(f)$ is the quantization error power spectral density, $\Delta$ is the quantization resolution, $f_s$ is the sampling frequency, and OSR represents the oversampling ratio [@pavan2017understanding].

Note that the SNR for a 1-bit ADC is 7.78 dB (6.02 + 1.76). Each factor-of-4 oversampling increases the SNR by 6 dB, and every 6 dB increase corresponds to gaining one additional bit of resolution. A 1-bit ADC with 24× oversampling can achieve a resolution of four bits. However, achieving 14-bit resolution would require oversampling by a factor of $4^{15}$, which is impractical [@integrated2003demystifying]. 

$\Delta\Sigma$ converters address this limitation through noise shaping, a technique that enables a gain of more than 6 dB per factor-of-4 oversampling [@integrated2003demystifying].

## Noise shaping

To further improve the conversion resolution at the same sampling frequency $f_s$ and with the same number of ADC bits, noise shaping can be applied. This is accomplished by high-pass filtering the quantization noise to displace most of its power from low frequencies, where the input signal spectrum resides, to higher frequencies near $f_s/2$ [@bajdechi2004systematic], ensuring that most of its power is moved outside the signal band [@delarosa2013].

To understand noise shaping, the system can be analyzed from different perspectives. Starting with @fig-first_order_delta_sigma_modulator_time_domain, the 1-bit DAC in the feedback loop can be observed. The primary role of the feedback DAC is to keep the integrator's average output near the comparator's reference level. The density of ones at the modulator output is proportional to the input signal. As the input increases, the comparator generates a higher number of ones, and conversely, for a decreasing input, fewer ones are produced. By summing the error voltage, the integrator acts as a low-pass filter for the input signal and a high-pass filter for the quantization noise. As a result, most of the quantization noise is shifted to higher frequencies [@integrated2003demystifying].

Another way to analyze the system, especially with regard to noise shaping, is by using linear models.

### Linear Model

@fig-Ideal-linear-model illustrates an idealized linear model of a delta-sigma modulator, represented as a negative feedback system. $x$ represents the discrete-time input signal to the system, $y$ is the system's output signal and $k_q$ refers to the gain factor of the quantization error and can be chosen arbitrarily in the case of a 1-bit quantizer. The model aims to improve the understanding that the goal is to have a frequency-dependent gain. This gain should be infinite at low frequencies so that the noise transfer function (NTF) has a small magnitude at low frequencies. The lowest-order system with these characteristics is an integrator. $\frac{1}{1 - z^{-1}}$ represents the discrete-time integrator. It accumulates the error signal over time and is a key element in shaping the noise spectrum. $z^{-1}$ is a unit delay element, which models the feedback delay in the loop [@pavan2017understanding].

![Ideal linear model](figures/Linear_model.drawio.svg){#fig-Ideal-linear-model width=80%}

At the heart of the model is the loop filter, a fundamental component that shapes the spectral characteristics of the quantization noise. Functionally, it acts as a discrete-time integrator, enabling past sample values to influence the current output. This feedback mechanism is essential for achieving noise shaping, as it suppresses in-band noise while pushing the quantization error toward higher frequencies, where it can be more easily filtered.

The following equation shows the relationship between the system's input, quantization error, and output:

$$
Y(z) = z^{-1} X(z) + (1 - z^{-1}) E(z).\tag{8}
$$

It illustrates that the output $Y(z)$ consists of the delayed input signal $X(z)$ and the shaped quantization error. Here, $z^{-1}$ represents the signal transfer function (STF), while the noise transfer function (NTF), $(1 - z^{-1})$, exhibits a first-order high-pass characteristic with a transmission zero at DC ($\omega = 0$, i.e., $z = e^{j\omega} = 1$), since $1 - e^{-j0} = 0$.

The logarithmic plot of the magnitude response of the NTF in log scale shows the first-order nature of the high-pass response, increasing at a rate of 20 dB per decade [@pavan2017understanding].

The in-band noise power decreases proportionally to $OSR^{-3}$.

$$
\text{IBN} = \frac{\Delta^2}{24\pi} \int_{-\frac{\pi}{OSR}}^{\frac{\pi}{OSR}} |(1 - e^{-j\omega})|^2 d\omega 
= \frac{\Delta^2}{12\pi} \int_{0}^{\frac{\pi}{OSR}} 4 \sin^2\left(\frac{\omega}{2}\right) d\omega
$$

$$
\approx \frac{\Delta^2}{12\pi} \int_{0}^{\frac{\pi}{OSR}} \omega^2 d\omega 
= \frac{\Delta^2}{36\pi} \frac{\pi^3}{OSR^3}.
$$

When the oversampling ratio is doubled, the in-band noise power reduces by 9 dB, which results in an effective resolution improvement of 1.5 bits. In contrast, basic oversampling without noise shaping only increases resolution by 0.5 bits for each doubling of the OSR. In principle, achieving high accuracy is possible by selecting a sufficiently large $OSR$. However, combining oversampling with noise shaping significantly reduces the required OSR value.

<!--
![Single chain with first-order Delta-Sigma loop](figures/11Mod.drawio.svg){#fig-Single-chain-with-first-order-Delta-Sigma-loop width=80%}

The system diagram of a first-order Sigma-Delta converter is illustrated in @fig-Single-chain-with-first-order-Delta-Sigma-loop. The output consists of the input signal $x$ combined with the quantization error, defined as $e = y - x$. Consequently, 

$$
Y(z) = z^{-1} X(z) + (1 - z^{-1}) E(z). \tag{8}
$$

The quantization error is attenuated in the signal band through first-order noise shaping, characterized by the transfer function $(1 - z^{-1})$, which has a zero at DC [@pavan2017understanding].
-->

## Second-Order Delta-Sigma Modulation

Following oversampling and noise shaping, the question arises as to how quantization noise can be further reduced. A modulator that integrates the input signal twice instead of just once is an effective way for that. @fig-second-order-delta-sigma-modulator illustrates a 1-bit, second-order modulator that uses two integrators instead of one. In this case, the noise term depends not only on the previous error but also on the two preceding errors.

![Second-order $\Delta\Sigma$ modulator](figures/22Mod.drawio.svg){#fig-second-order-delta-sigma-modulator width=100%}

Increasing the resolution and the effective number of bits (ENOB) in a $\Delta\Sigma$ modulator can be achieved by incorporating an additional integrator and feedback path. A linearized analysis in this scenario leads to the expression:

$$
Y(z) = z^{-1} X(z) + (1 - z^{-1})^2E(z). \tag{9}
$$

This shows that the NTF takes the form $(1 - z^{-1})^2$ in the z-domain, applying a shaping function of $(2 \sin(\omega/2))^4$ to the PSD. As a result, doubling the OSR leads to an increase of approximately 2.5 bits in resolution. This represents a significantly better trade-off compared to a first-order modulator.

By incorporating more integrators and feedback branches within the loop, it is possible to achieve higher-order noise transfer functions. In the case of an L-th order loop filter, the NTF follows the form:

$$ 
NTF(z) = (1 - z^{-1})^L. \tag{10}
$$ 

Under these conditions, the in-band noise power is given by:

$$ 
IBN = \frac{\pi^{2L} e^2_{rms}}{(2L + 1) OSR^{2L+1}}. \tag{11}
$$

::: {.callout-important title="The Trade-Offs of High-Order $\Delta\Sigma$ Modulators"}

The previous discussion suggests that a $\Delta\Sigma$ loop with a carefully chosen, very high-order NTF could, in theory, achieve exceptionally high performance. However, if something sounds too good to be true, it probably is. In practice, high-order loops introduce stability challenges. These limitations reduce the achievable resolution to a lower value than the theoretical predictions.

Moreover, second- and higher-order modulators come with additional drawbacks, including increased complexity, multiple feedback loops, and greater design difficulty.

:::

## Decimation

After the $\Delta\Sigma$ modulator, the decimator follows, which is shown as the final stage in @fig-General-block-diagram-of-a-Delta-sigma-ADC. The undesirable characteristics of the modulator output include high-frequency noise and a high-speed, 1-bit output rate. Once the signal is in the digital domain, a low-pass digital filter can be applied to attenuate the high-frequency noise, while a down-sampler can be used to reduce the output data rate [@baker2011delta]. 

A down-sampler with a downsampling factor $M$, where $M$ is a positive integer, produces an output sequence with a sampling rate that is ${1}/{M}$ of the input sequence's sampling rate. It retains every $M$-th sample of the input sequence and discards the $M - 1$ intermediate samples to generate the output. By removing samples, down-sampling increases the sampling period. If the input sampling rate is defined as $F_T = {1}/{T}$, then the output sampling rate $F_M$ is related to $F_T$ as follows [@wolter2023dsp2]:

$$
F_M = \frac{1}{T_M} = \frac{1}{M T} = \frac{F_T}{M}. \tag{12}
$$

This means that after down-sampling, the new sampling frequency is a fraction of the original rate. It is also important to examine the frequency-domain relation of a down-sampler. The relationship between input and output in the z-transform domain can be analyzed by the following relation:  
  
$$
Y(z) = \frac{1}{M} \sum_{k=0}^{M-1} X\left(z^{1/M} W_M^k\right). \tag{13}
$$
  
This equation shows that the output spectrum consists of a sum of $M$ stretched and shifted copies of the input spectrum, scaled by a factor of $1/M$.  

The output spectrum of a down-sampler with a factor of $M$ is given by:  
 
$$
Y(e^{j\omega}) = \frac{1}{M} \sum_{k=0}^{M-1} X\left(e^{j(\omega - 2\pi k)/M}\right). \tag{14}
$$
 
Aliasing in $Y(e^{j\omega})$ is completely avoided if and only if:  
  
$$
X(e^{j\omega}) = 0 \quad \text{for} \quad {\pi}/{M} \leq |\omega| \leq \pi . \tag{15}
$$
 
This means that $x[n]$ must be band-limited to $\pm \pi / M$ [@wolter2023dsp2].

In multistage signal processing systems, the order of **downsampling and filtering** can be interchanged under certain conditions. This is particularly useful as it allows for computational efficiency in systems with variable sampling rates. A signal $x[n]$ is first downsampled by a factor of $M$ and then processed by the filter $H(z)$:

![Multirate Idenetity: Downsampling before filtering](figures/MultirateIdentity.drawio.svg){#fig-Multirate-Identitaet1-Downsampling-before-filtering width=65%}

The corresponding representation in the Z-domain is:

$$
Y_1(z) = H(z) \frac{1}{M} \sum_{k=0}^{M-1} X\left(z^{1/M} W_M^k\right).
$$

The order of filtering and downsampling can be interchanged if the original filter $H(z)$ is replaced with $H(z^M)$:

![Multirate Idenetity: Filtering before downsampling](figures/MultirateIdentity2.drawio.svg){#fig-Multirate-Identitawt:-Filtering-before-downsampling width=65%}

This equivalence allows for flexibility in system design.

The down-sampler retains only every $M$-th sample. Consequently, it is sufficient to compute only for values that are multiples of $M$, while skipping the computations of the intermediate samples. This results in a computational complexity reduction by a factor of $M$. The decimation filter can be implemented as either an FIR or an IIR filter. Since the output is being downsampled, it is sufficient to compute only for values of $n$ that are integer multiples of $M$. However, the feedback signal must still be computed for all values of $n$. As a result, in such cases, the computational savings are always less than $M$. Therefore, FIR filters are often preferred in multirate systems due to their advantages in stability and implementation efficiency.

### Two-Stage Decimator Structure
As previously mentioned, interchanging the positions of individual branches in such a cascade can often lead to a more computationally efficient implementation. The basic building blocks for altering the sampling rate only allow for an integer factor change in the sampling rate of a signal. To achieve a fractional (rational) change in the sampling rate, a cascade consisting of a downsampler with factor $M$ and an upsampler with factor $L$ must be used. These rules allow us to reposition the fundamental sampling rate conversion devices within multirate networks to more favorable locations. They prove to be extremely useful in the design and analysis of more complex systems [@mitra2001digital]. 

<!--
The specifications for the lowpass decimation filter can be developed as follows:

$$
|H(e^{j\omega})| =
\begin{cases}
1, & |\omega| \le \frac{\omega_p}{M},\\
0, & \frac{\pi}{M} \le |\omega| \le \pi
\end{cases}
$$

where $\omega_c$ denotes the highest frequency that needs to be preserved in the decimated signal.
-->

<!--The lowpass decimation filter may be implemented using either an FIR or an IIR digital filter. In typical single-rate digital signal processing scenarios, IIR filters tend to be the more computationally efficient choice compared to FIR filters. As a result, they are often selected when reducing computational complexity is a key consideration.-->

For the decimation filter the computational savings can be achieved, especially with FIR filters. With IIR filters, the simplification is only partially applicable. While only every $M$-th output is used, the internal recursive nature of the filter requires the computation of all intermediate values. This limits the potential efficiency gain. However, IIR filters can still be more efficient overall due to their typically lower required filter order compared to FIR filters.

The following example is intended to demonstrate how to design a decimator for specific specifications. It is important to understand the concept behind systems engineering, so that the decimator can be implemented for different parameter values as well. In this case, a decimator is designed for the system in order to reduce the sampling rate of the signal from **220 kHz** to **430 Hz**. The specifications are assumed to be as follows:

| Specification                    | Value            |
|----------------------------------|------------------|
| Sampling Rate ($f_s$)       | 220 kHz          |
| Output Sampling Rate ($f_d$) | 430 Hz           |
| Passband Edge ($f_p$)             | 215 Hz           |
| Stopband Edge ($f_{sb}$)             | 250 Hz           |
| Passband Ripple ($\delta_p$)      | 0.002            |
| Stopband Ripple ($\delta_s$)      | 0.001            |
| OSR      | 512            |

The desired decimator with a down-sampling factor $M$ = 512 is shown below.

![Desired Decimator with a down-sampling factor of $M$ = 512](figures/DesiredDecimator.drawio.svg){#fig-desired-decimator width=65%}

Since the signal of interest occupies the range $[-\pi/\text{OSR}, \pi/\text{OSR}]$, it can be down-sampled by a factor of OSR without introducing aliasing. This results in a sequence that is sampled at the Nyquist rate [@pavan2017understanding], where the output sampling frequency is $f_s = 2 \cdot f_{\text{sig,max}} = 430\,\text{Hz}$.

The meaningful variables in this overall system are the modulator's sampling rate $f_s$ and the digital/decimation filter's output-data rate $f_d$. The ratio between these two variables is defined as the decimation ratio (DR). In this example, the decimation ratio (DR) is 512, which is the result of dividing the modulator's sampling rate of 220 kHz by the output data rate of 430 Hz. A high DR reduces quantization noise and results in a higher effective number of bits (ENOB), whereas a low DR leads to increased noise and lower ENOB.

The filter order $N$ (i.e., length $N+1$) must first be estimated. The required order $N$ can be calculated using the following equation:

$$
N = \frac{-20 \log_{10} \left( \sqrt{\delta_p \delta_s} \right) - 13}{14.6 \Delta f} \tag{16}, 
$$

where $\Delta f = (f_{sb} - f_p) / f_s$ is the normalized transition bandwidth. Alternatively, the filter order of $H(z)$ can also be estimated using the `firpmord` function. For these specifications, the filter order is $N = 19138$. Thus, the number of multiplications per second required for the decimation filter is given by:

$$
R_{M,H} = 19138 \cdot 430 = 8{,}229{,}340\ \text{mult/sec.}
$$

The obvious problem with this approach is that it is extremely computationally intensive and therefore highly inefficient. It would lead to high power consumption, which is particularly disadvantageous for embedded systems, as well as increased resource usage in general. For this reason, a step-by-step design of a two-stage decimator is considered. First, the decimation filter $H(z)$ is implemented using an IFIR filter. The decimation factor $M$ is factorized as $512 = 256 \cdot 2$, allowing the interpolation factor $L$ for $F(z^L)$ to be chosen as $L = 256$. By exploiting the multirate identity, the final two-stage decimator design is derived.

The overall ripple of the filter cascade is given by the sum of the two passband ripples. This can be compensated by designing $G(z)$ and $F(z)$ such that each has an equal passband ripple of $\delta_p = 0.001$ (rather than 0.002). The cascade of $G(z)$ and $F(z)$ achieves a stopband performance at least as good as either $F(z)$ or $G(z)$ individually. Therefore, a stopband ripple of $\delta_s = 0.001$ can be selected for both filters. This corresponds to the specifications of $H(z)$ stretched by a factor of $L = 256$.

As a result, the filter $F(z)$ has:

$$
f_p = 256 \cdot 215\ \text{Hz} = 55{,}040\ \text{Hz},
$$

$$
f_{sb} = 256 \cdot 250\ \text{Hz} = 64{,}000\ \text{Hz}.
$$

For the stopband edge of $G(z)$:

$$
f_s / 256 - f_{sb} = \left({220,000}/{256} - 250\right)\ \text{Hz} = 609\ \text{Hz}.
$$

The frequency specifications of $F(z)$ and $G(z)$ are summarized as follows:

- $F(z):\ f_p = 55.04\ \text{kHz},\ f_{sb} = 64\ \text{kHz},$
- $G(z):\ f_p = 215\ \text{Hz},\ f_{sb} = 609\ \text{Hz}.$

The filter orders of $F(z)$ and $G(z)$ estimated using the `firpmord` function are $N_F = 80$ and $N_G = 1818$.

The implementation of $F(z)$ followed by a factor-of-2 down-sampler requires:

$$
R_{M,F} = 80 \cdot 430 = 34{,}400\ \text{mult/sec}.
$$

The implementation of $G(z)$ followed by a factor-of-256 down-sampler requires:

$$
R_{M,G} = 1818 \cdot 860 = 1{,}563{,}480\ \text{mult/sec}.
$$

Therefore, the savings compared to the single-filter design are approximately **81%**.

Below is an illustration summarizing the steps in the two-stage realization of the decimator structure.

![The steps in the two-stage realization of the decimator structure](figures/MultiStage.drawio.svg){#fig-The-steps-in-the-two-stage-realization-of-the-decimator-structure width=80%}